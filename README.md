# Semantic-Segmentation
## An algorithm that self-driving cars can use to percieve surroundings and potentially interact effectively and safely with their environments.
#### (If you have trouble viewing the notebook, view it here: https://nbviewer.org/github/tzviblonder/Semantic-Segmentation/blob/main/Semantic%20Segmentation%20for%20Self-Driving%20Cars.ipynb)
#### Semantic segmentation is the task of segmenting an image into different categories. In such a case, each pixel is labeled with the class it helps represent in the image. Semantic segmentation is part of the field of computer vision and has applications in autonomous vehicles, medical imaging, and more. 
#### This algorithm uses a u-net model, a kind of architecture proposed in the paper "U-Net: Convolutional Networks for Biomedical Image Segmentation" by Ronneberger, Fischer, & Brox (2015) which can be viewed here https://arxiv.org/abs/1505.04597.
#### A u-net model is a convolutional neural network characterized by a series of convolutional blocks (which consist of convolutional and max pooling layers) that reduce the dimensionality of the images, encoding them in a deeper but otherwise much smaller tensor, followed by a corresponding series of convolutional transpose blocks (which have convolutional transpose, upsampling, and regular convolutional layers) that rebuild the encoded image into its corresponding mask. When visualized, this model is shaped like a U; hence the name. The output is a 300x400x13 tensor in which each pixel gives a probability distribution over a vector of size 13; the place with the highest probability is the predicted class.
#### Another important feature of the u-net model is the skip connections. In the convolutional blocks in the first half of the model, each convolutional layer is copied and set aside before going through max pooling. In the second half, when the encoded images are being grown and built into the mask, the saved layers are concatenated with the results of the convolutional transpose layers with equivalent numbers of filters. For example, the convolutional layer with 32 filters towards the beginning of the model is concatenated with the 32-filter convolutional transpose layer at the end of the model, and so on.
#### The advantage of using these skip connections is two-fold: First, the non-linear structure prevents the problem of vanishing gradients. When the gradient flow comes from multiple sources, it is much less likely to quickly approach zero. Second, the skip connections enable information from the beginning of the model to move quickly to the end without getting swallowed up in too many layers. This ensures that little data gets lost, as well as creating a divergence in training by which simpler image features (such as the sky or the road) can be modeled with the skip connection (i.e. a much smaller neural network) and more complex features (such as people and traffic lights) can take a longer route through the model and have enough space to train.
